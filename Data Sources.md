# Terraform Data Sources: Complete Guide â€” Beginner to Production-Ready

---

# ðŸ“˜ BEGINNER LEVEL

## What Are Data Sources?

**In simple terms:** Data sources let Terraform **read** information from existing infrastructure without **creating or managing** it. They're read-only lookups.

Think of it this way â€” Terraform has two modes:

```
resource  â†’  CREATE / UPDATE / DESTROY  (Terraform owns it)
data      â†’  READ ONLY                  (Terraform just looks it up)
```

**Why are they used?**
- Reference infrastructure created outside Terraform (manually, by another team, another Terraform project)
- Look up dynamic values you can't hardcode (latest AMI ID, current account ID, existing VPC)
- Share information between separate Terraform projects (state files)
- Avoid hardcoding values that change over time or across environments

**Real-World Analogy:**

> Imagine you're a **new employee joining a company**. You don't build the office building â€” it already exists. But you need information about it: the WiFi password, the room numbers, the security access codes.
>
> You **look these up** from the company directory â€” you don't create them, you don't own them, you just read them and use them in your work.
>
> That's exactly what data sources do. The existing infrastructure is the office building. The data source is your lookup query. Your resource blocks use what you looked up.

---

## Basic Syntax

```hcl
# Resource block   â†’ creates infrastructure
resource "aws_instance" "my_server" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
}

# Data source block â†’ reads existing infrastructure
data "aws_instance" "existing_server" {
  instance_id = "i-0a1b2c3d4e5f67890"
}

# General syntax
data "<PROVIDER_TYPE>" "<LOCAL_NAME>" {
  # Filter arguments to find the right resource
  filter_argument = "value"
}

# Reference the data source result
output "existing_server_ip" {
  value = data.aws_instance.existing_server.public_ip
  #       ^^^^                                         â† always starts with "data."
  #            ^^^^^^^^^^^                             â† provider type
  #                        ^^^^^^^^^^^^^^^             â† your local name
  #                                       ^^^^^^^^^    â† attribute you want
}
```

---

## Minimal Working Example (No Cloud Needed)

```hcl
# main.tf â€” Uses local provider only

terraform {
  required_providers {
    local = {
      source  = "hashicorp/local"
      version = "~> 2.4"
    }
  }
}

# STEP 1: Create a file (pretend this was created by someone else)
resource "local_file" "config" {
  filename = "./existing-config.txt"
  content  = "database_host=prod-db.internal\ndatabase_port=5432"
}

# STEP 2: Use a data source to READ that file back
data "local_file" "read_config" {
  filename   = "./existing-config.txt"
  depends_on = [local_file.config]  # Ensure file exists first
}

# STEP 3: Use the data in another resource
resource "local_file" "app_settings" {
  filename = "./app-settings.txt"
  content  = <<-EOT
    # App Settings â€” Generated by Terraform
    # Source config contents:
    ${data.local_file.read_config.content}
    app_name=myapp
  EOT
}

output "config_contents" {
  value = data.local_file.read_config.content
}
```

```bash
terraform init
terraform apply -auto-approve

# Output:
# config_contents = "database_host=prod-db.internal\ndatabase_port=5432"
```

---

## Common Beginner Mistakes

```hcl
# âŒ MISTAKE 1: Confusing data source with resource
data "aws_vpc" "main" {    # âŒ This does NOT create a VPC!
  default = true
}
# âœ… It only READS an existing VPC

# âŒ MISTAKE 2: Wrong reference prefix (forgetting "data.")
data "aws_ami" "ubuntu" {
  most_recent = true
}
resource "aws_instance" "web" {
  ami = aws_ami.ubuntu.id          # âŒ ERROR â€” missing "data." prefix
  ami = data.aws_ami.ubuntu.id     # âœ… CORRECT
}

# âŒ MISTAKE 3: Data source returning multiple results (no filter)
data "aws_vpc" "all_vpcs" {
  # No filters â€” matches ALL VPCs
  # ERROR: multiple VPCs found, use filters to narrow down
}
# âœ… Add filters to get exactly one result:
data "aws_vpc" "main" {
  filter {
    name   = "tag:Name"
    values = ["production-vpc"]
  }
}

# âŒ MISTAKE 4: Hardcoding values that data sources can provide
resource "aws_instance" "web" {
  ami = "ami-0c55b159cbfafe1f0"   # âŒ This AMI ID is region-specific
}                                  #    and becomes outdated quickly!
# âœ… Use data source to always get the latest:
data "aws_ami" "amazon_linux" {
  most_recent = true
  owners      = ["amazon"]
  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }
}
resource "aws_instance" "web" {
  ami = data.aws_ami.amazon_linux.id  # âœ… Always current, always correct region
}

# âŒ MISTAKE 5: Referencing data source output that doesn't exist
data "aws_s3_bucket" "my_bucket" {
  bucket = "my-company-data"
}
output "bucket_name" {
  value = data.aws_s3_bucket.my_bucket.name   # âŒ attribute is "bucket", not "name"
  value = data.aws_s3_bucket.my_bucket.bucket # âœ… Check provider docs for exact attrs
}
```

---

# ðŸ“— INTERMEDIATE LEVEL

## How Data Sources Work Internally

Understanding the **execution lifecycle** of data sources is critical:

```
terraform plan / terraform apply
        â†“
1. Terraform evaluates ALL data source blocks
        â†“
2. For each data source, Terraform calls the provider API
   (e.g., AWS DescribeVpcs, DescribeImages API calls)
        â†“
3. Provider filters results using your filter arguments
        â†“
4. If exactly 1 result â†’ success, result stored in plan
   If 0 results       â†’ ERROR: no matching resource found
   If 2+ results      â†’ ERROR: multiple resources found
        â†“
5. Data source attributes become available for use
   in resource blocks, locals, outputs
        â†“
6. During apply, data sources are re-evaluated
   (unless result is already known from plan)
```

**Key timing insight:**

```hcl
# Data sources evaluated at PLAN TIME
# This means you can use data source values to decide what resources to create

data "aws_vpc" "main" {
  filter {
    name   = "tag:Environment"
    values = [var.environment]
  }
}

# At plan time, Terraform knows the VPC CIDR and can use it immediately
resource "aws_subnet" "app" {
  vpc_id     = data.aws_vpc.main.id
  cidr_block = cidrsubnet(data.aws_vpc.main.cidr_block, 8, 1)
  #                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  #                       Used at plan time â€” Terraform computes this
}
```

---

## The Most Important Data Sources (AWS)

### 1. aws_ami â€” Find the Right Machine Image

```hcl
# ============================================
# Find Latest Amazon Linux 2 AMI
# ============================================
data "aws_ami" "amazon_linux_2" {
  most_recent = true       # Among all matches, get newest
  owners      = ["amazon"] # Only AMIs owned by Amazon

  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]  # Glob patterns work!
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }

  filter {
    name   = "state"
    values = ["available"]
  }
}

# Find Latest Ubuntu 22.04 LTS
data "aws_ami" "ubuntu_22" {
  most_recent = true
  owners      = ["099720109477"]  # Canonical's AWS Account ID

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
  }
}

# Find Custom Internal AMI (built by your pipeline)
data "aws_ami" "app_golden_image" {
  most_recent = true
  owners      = ["self"]  # Only YOUR account's AMIs

  filter {
    name   = "tag:Project"
    values = ["myapp"]
  }

  filter {
    name   = "tag:Validated"
    values = ["true"]
  }
}

# Use in a resource
resource "aws_instance" "web" {
  ami           = data.aws_ami.amazon_linux_2.id  # Always latest
  instance_type = "t2.micro"
}

# What attributes are available?
output "ami_details" {
  value = {
    id           = data.aws_ami.amazon_linux_2.id
    name         = data.aws_ami.amazon_linux_2.name
    description  = data.aws_ami.amazon_linux_2.description
    creation_date = data.aws_ami.amazon_linux_2.creation_date
    root_device  = data.aws_ami.amazon_linux_2.root_device_type
    architecture = data.aws_ami.amazon_linux_2.architecture
  }
}
```

### 2. aws_vpc / aws_subnet â€” Find Network Resources

```hcl
# ============================================
# Find Existing VPC
# ============================================

# By tag
data "aws_vpc" "production" {
  filter {
    name   = "tag:Name"
    values = ["production-vpc"]
  }
}

# By CIDR block
data "aws_vpc" "internal" {
  cidr_block = "10.0.0.0/16"
}

# The default VPC (every AWS account has one)
data "aws_vpc" "default" {
  default = true
}

# ============================================
# Find Subnets in that VPC
# ============================================

# Get ONE specific subnet by tag
data "aws_subnet" "public_1" {
  filter {
    name   = "tag:Name"
    values = ["production-public-us-east-1a"]
  }
}

# Get ALL subnets matching a filter (returns a SET of IDs)
data "aws_subnets" "private" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.production.id]
  }

  filter {
    name   = "tag:Tier"
    values = ["private"]
  }
}

# Use the subnet IDs list in another resource
resource "aws_autoscaling_group" "app" {
  vpc_zone_identifier = data.aws_subnets.private.ids  # List of subnet IDs
  min_size            = 2
  max_size            = 10
  # ...
}

output "private_subnet_ids" {
  value = data.aws_subnets.private.ids
  # Result: ["subnet-abc", "subnet-def", "subnet-ghi"]
}
```

### 3. aws_caller_identity â€” Who Am I?

```hcl
# ============================================
# Get Current AWS Account Info
# ============================================
data "aws_caller_identity" "current" {}
# No arguments needed â€” just asks "who is calling?"

data "aws_region" "current" {}
# Gets the current provider region

data "aws_availability_zones" "available" {
  state = "available"  # Only AZs that are available (not impaired)
}

# Use them to build dynamic ARNs and names
locals {
  account_id  = data.aws_caller_identity.current.account_id
  region      = data.aws_region.current.name
  user_arn    = data.aws_caller_identity.current.arn

  # Build resource ARN dynamically (works in any account/region!)
  bucket_arn = "arn:aws:s3:::${var.bucket_name}"

  lambda_arn = "arn:aws:lambda:${local.region}:${local.account_id}:function:${var.function_name}"
}

# Deploy to all available AZs automatically
resource "aws_subnet" "public" {
  count             = length(data.aws_availability_zones.available.names)
  vpc_id            = aws_vpc.main.id
  cidr_block        = cidrsubnet("10.0.0.0/16", 8, count.index)
  availability_zone = data.aws_availability_zones.available.names[count.index]
  # Automatically distributes across us-east-1a, 1b, 1c, 1d, 1e, 1f
}

output "account_info" {
  value = {
    account_id = data.aws_caller_identity.current.account_id
    region     = data.aws_region.current.name
    caller_arn = data.aws_caller_identity.current.arn
  }
}
```

### 4. aws_secretsmanager_secret â€” Fetch Secrets Safely

```hcl
# ============================================
# Fetch Secrets Without Storing in Code
# ============================================

# Fetch the secret metadata
data "aws_secretsmanager_secret" "db_credentials" {
  name = "production/myapp/database"
}

# Fetch the actual secret value
data "aws_secretsmanager_secret_version" "db_credentials" {
  secret_id = data.aws_secretsmanager_secret.db_credentials.id
}

# Parse JSON secret
locals {
  db_creds = jsondecode(
    data.aws_secretsmanager_secret_version.db_credentials.secret_string
  )
  # Assuming secret is: {"username":"admin","password":"s3cr3t","host":"db.internal"}
}

resource "aws_db_instance" "app" {
  identifier = "myapp-production"
  engine     = "postgres"
  username   = local.db_creds.username   # âœ… From Secrets Manager
  password   = local.db_creds.password   # âœ… Never in code or state!
  # ...
}

# Or use SSM Parameter Store (cheaper than Secrets Manager)
data "aws_ssm_parameter" "db_password" {
  name            = "/production/myapp/db-password"
  with_decryption = true   # Decrypt SecureString parameters
}

resource "aws_db_instance" "app" {
  password = data.aws_ssm_parameter.db_password.value
}
```

### 5. terraform_remote_state â€” Cross-Project Data Sharing

```hcl
# ============================================
# Share Data Between Terraform Projects
# ============================================

# PROJECT 1: networking/ â€” Creates the VPC
# outputs.tf in networking project:
output "vpc_id" {
  value = aws_vpc.main.id
}
output "private_subnet_ids" {
  value = aws_subnet.private[*].id
}


# PROJECT 2: application/ â€” Uses the VPC from networking project
data "terraform_remote_state" "networking" {
  backend = "s3"
  config = {
    bucket = "my-company-terraform-state"
    key    = "networking/terraform.tfstate"
    region = "us-east-1"
  }
}

# Now you can use any OUTPUT from the networking project:
resource "aws_instance" "app_server" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t2.micro"
  subnet_id     = data.terraform_remote_state.networking.outputs.private_subnet_ids[0]
  # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  # Reads VPC output from a completely separate Terraform project!
}

resource "aws_security_group" "app" {
  vpc_id = data.terraform_remote_state.networking.outputs.vpc_id
}
```

---

## Data Source Filters â€” Deep Dive

```hcl
# ============================================
# How Filters Work
# ============================================

# Single filter â€” must match ALL conditions (AND logic)
data "aws_instances" "web_servers" {
  filter {
    name   = "tag:Role"           # Tag key
    values = ["web"]              # Tag value must be "web"
  }

  filter {
    name   = "instance-state-name"
    values = ["running"]          # Must also be running
  }
  # Result: instances that are tagged Role=web AND are running
}

# Multiple values in one filter = OR logic within that filter
data "aws_instances" "app_or_web" {
  filter {
    name   = "tag:Role"
    values = ["web", "api"]   # Role=web OR Role=api
  }
  filter {
    name   = "instance-state-name"
    values = ["running"]      # AND must be running
  }
}

# Common filter names for AWS resources:
# "tag:<TagKey>"          â†’ Filter by tag value
# "vpc-id"               â†’ Filter by VPC
# "availability-zone"    â†’ Filter by AZ
# "state"                â†’ Filter by state
# "instance-type"        â†’ Filter by instance size
# "name"                 â†’ Filter by name (often supports wildcards)
```

---

## Best Practices

```hcl
# âœ… BEST PRACTICE 1: Always use specific filters â€” avoid ambiguity
data "aws_vpc" "bad" {}  # âŒ No filter â€” fails if multiple VPCs exist

data "aws_vpc" "good" {  # âœ… Specific filter
  filter {
    name   = "tag:Name"
    values = ["${var.environment}-vpc"]
  }
}

# âœ… BEST PRACTICE 2: Use data sources for account/region info
# Never hardcode account IDs or regions
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}

resource "aws_iam_policy" "bad" {
  policy = jsonencode({
    Statement = [{
      Resource = "arn:aws:s3:::my-bucket-123456789"  # âŒ Hardcoded account!
    }]
  })
}

resource "aws_iam_policy" "good" {
  policy = jsonencode({
    Statement = [{
      Resource = "arn:aws:s3:::my-bucket-${data.aws_caller_identity.current.account_id}"  # âœ…
    }]
  })
}

# âœ… BEST PRACTICE 3: Use most_recent = true for AMIs
# Never hardcode AMI IDs â€” they're region-specific and expire
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
  }
}

# âœ… BEST PRACTICE 4: Use remote_state for cross-team sharing
# Don't duplicate infrastructure â€” read other team's outputs
data "terraform_remote_state" "networking" {
  backend = "s3"
  config = {
    bucket = "company-terraform-state"
    key    = "shared/networking/terraform.tfstate"
    region = "us-east-1"
  }
}

# âœ… BEST PRACTICE 5: Validate data source results with locals
data "aws_vpc" "main" {
  filter {
    name   = "tag:Name"
    values = ["${var.environment}-vpc"]
  }
}

locals {
  # Compute once, use everywhere â€” avoid re-referencing the data source
  vpc_id   = data.aws_vpc.main.id
  vpc_cidr = data.aws_vpc.main.cidr_block
}
```

---

## Interview-Focused Points

> **Q: What is a data source in Terraform and how does it differ from a resource?**
> A resource creates, updates, and destroys infrastructure â€” Terraform manages its lifecycle. A data source only reads existing infrastructure â€” Terraform never modifies it. Resources use `resource` block; data sources use `data` block and are referenced with `data.<type>.<name>.<attr>`.

> **Q: When would you use terraform_remote_state vs a data source?**
> Use `terraform_remote_state` to read outputs from another Terraform state file (inter-project sharing). Use provider-specific data sources (like `aws_vpc`) to read infrastructure that exists regardless of whether Terraform created it. Remote state creates tight coupling between projects â€” many teams prefer publishing data via SSM Parameter Store or similar instead.

> **Q: At what point in the Terraform lifecycle are data sources evaluated?**
> Data sources are evaluated during `terraform plan` â€” they make live API calls at plan time. This means the values are available immediately for resource block expressions. However, if a data source depends on a resource that doesn't exist yet, it can only be evaluated during `apply` (known after apply).

> **Q: What happens if a data source filter matches 0 or 2+ resources?**
> Terraform raises an error and stops. If 0 results: "No matching resource found." If 2+ results: "Multiple resources found, use a more specific filter." You must ensure your filters are precise enough to return exactly one result.

> **Q: How do you share data between separate Terraform state files?**
> Three patterns: (1) `terraform_remote_state` data source reads another project's outputs directly. (2) Store values in SSM Parameter Store and read with `aws_ssm_parameter` data source. (3) Store in a data store like Consul or DynamoDB. Remote state is simplest but creates coupling; SSM/Consul is more loosely coupled.

---

# ðŸ“• ADVANCED LEVEL

## Deep Dive: Data Source Evaluation Timing

One of the most subtle but important concepts in Terraform is **when** data sources are evaluated. There are three scenarios:

```hcl
# ============================================
# SCENARIO 1: Data source with static filter
# (Evaluated at plan time â€” always works)
# ============================================
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
  }
}
# âœ… Filter values are known BEFORE apply
# Terraform can fetch the AMI ID during terraform plan


# ============================================
# SCENARIO 2: Data source depending on a resource
# (Evaluated during apply â€” "known after apply")
# ============================================
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
}

data "aws_vpc" "lookup" {
  id = aws_vpc.main.id  # â† This value doesn't exist until apply!
}

resource "aws_subnet" "app" {
  vpc_id     = data.aws_vpc.lookup.id
  cidr_block = cidrsubnet(data.aws_vpc.lookup.cidr_block, 8, 1)
  # âš ï¸ During plan: cidr_block shows as "(known after apply)"
  # âœ… During apply: Terraform creates VPC first, then evaluates data source
}

# In this case â€” just reference the resource directly!
resource "aws_subnet" "app_better" {
  vpc_id     = aws_vpc.main.id          # âœ… Simpler â€” no data source needed
  cidr_block = cidrsubnet(aws_vpc.main.cidr_block, 8, 1)
}


# ============================================
# SCENARIO 3: Data source in a depends_on chain
# Forces deferred evaluation (Terraform 0.13+)
# ============================================
resource "aws_iam_role" "lambda" {
  name = "lambda-role"
  # ...
}

resource "aws_iam_role_policy_attachment" "lambda_policy" {
  role       = aws_iam_role.lambda.name
  policy_arn = "arn:aws:iam::aws:policy/AWSLambdaBasicExecutionRole"
}

# This data source needs to see the role AFTER the policy is attached
data "aws_iam_role" "lambda_with_policies" {
  name       = aws_iam_role.lambda.name
  depends_on = [aws_iam_role_policy_attachment.lambda_policy]
  # âš ï¸ depends_on on a data source forces it to be evaluated during APPLY
  # not during plan â€” values will be "known after apply" in the plan
}
```

## Advanced: Dynamic Data Sources with for_each

```hcl
# ============================================
# Multiple Data Sources Dynamically
# ============================================

# Look up multiple VPCs at once
variable "vpc_names" {
  default = {
    production = "production-vpc"
    staging    = "staging-vpc"
    dev        = "dev-vpc"
  }
}

# âš ï¸ Data sources support for_each too!
data "aws_vpc" "environments" {
  for_each = var.vpc_names

  filter {
    name   = "tag:Name"
    values = [each.value]
  }
}

# Access each VPC
output "vpc_ids" {
  value = {
    for env, vpc in data.aws_vpc.environments :
    env => vpc.id
  }
}
# Result: { "production" = "vpc-abc", "staging" = "vpc-def", "dev" = "vpc-ghi" }

# Look up subnets for each VPC dynamically
data "aws_subnets" "private" {
  for_each = data.aws_vpc.environments

  filter {
    name   = "vpc-id"
    values = [each.value.id]
  }

  filter {
    name   = "tag:Tier"
    values = ["private"]
  }
}

output "all_private_subnet_ids" {
  value = {
    for env, subnets in data.aws_subnets.private :
    env => subnets.ids
  }
}
```

## Advanced: templatefile and file Data Functions

```hcl
# ============================================
# templatefile â€” Dynamic File Rendering
# ============================================

# userdata.sh.tpl â€” Template file
# #!/bin/bash
# echo "Starting ${app_name} in ${environment}"
# aws s3 cp s3://${config_bucket}/config.json /etc/app/config.json
# systemctl start ${app_name}

resource "aws_instance" "app" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t3.micro"

  user_data = templatefile("${path.module}/templates/userdata.sh.tpl", {
    app_name      = var.app_name
    environment   = var.environment
    config_bucket = aws_s3_bucket.config.bucket
  })
}

# ============================================
# file() function â€” Read a file at plan time
# ============================================
resource "aws_key_pair" "deployer" {
  key_name   = "deployer-key"
  public_key = file("${path.module}/keys/deployer.pub")
  # Reads the SSH public key file â€” simpler than templatefile for static files
}

resource "aws_iam_policy" "app" {
  name   = "app-policy"
  policy = file("${path.module}/policies/app-policy.json")
  # Reads JSON policy file â€” cleaner than inline jsonencode for complex policies
}
```

## Advanced: The http Data Source

```hcl
# ============================================
# http â€” Fetch data from any HTTP endpoint
# ============================================
terraform {
  required_providers {
    http = {
      source  = "hashicorp/http"
      version = "~> 3.0"
    }
  }
}

# Fetch your current public IP (useful for whitelisting CI runners)
data "http" "my_public_ip" {
  url = "https://ipv4.icanhazip.com"
}

resource "aws_security_group" "ci_access" {
  name = "ci-runner-access"

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["${chomp(data.http.my_public_ip.response_body)}/32"]
    # chomp() removes trailing newline from the HTTP response
    description = "SSH from CI runner IP"
  }
}

# Fetch configuration from an internal API
data "http" "app_config" {
  url = "https://config-service.internal/api/v1/settings"

  request_headers = {
    Accept        = "application/json"
    Authorization = "Bearer ${var.config_api_token}"
  }
}

locals {
  app_settings = jsondecode(data.http.app_config.response_body)
}
```

## Advanced: aws_iam_policy_document â€” Generate IAM Policies

```hcl
# ============================================
# aws_iam_policy_document â€” Best way to write IAM policies
# ============================================

# This is technically a data source but works as a generator
data "aws_iam_policy_document" "app_permissions" {
  # Statement 1: S3 access
  statement {
    sid    = "S3ReadWrite"
    effect = "Allow"

    actions = [
      "s3:GetObject",
      "s3:PutObject",
      "s3:DeleteObject",
    ]

    resources = [
      aws_s3_bucket.app.arn,
      "${aws_s3_bucket.app.arn}/*",
    ]
  }

  # Statement 2: Secrets Manager read
  statement {
    sid    = "SecretsManagerRead"
    effect = "Allow"

    actions = ["secretsmanager:GetSecretValue"]

    resources = [
      "arn:aws:secretsmanager:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:secret:${var.environment}/myapp/*"
    ]
  }

  # Statement 3: Deny specific dangerous actions
  statement {
    sid    = "DenyDangerous"
    effect = "Deny"

    actions = [
      "s3:DeleteBucket",
      "s3:DeleteBucketPolicy",
    ]

    resources = ["*"]
  }
}

# Use the generated policy document
resource "aws_iam_policy" "app" {
  name   = "${var.environment}-app-policy"
  policy = data.aws_iam_policy_document.app_permissions.json
  # âœ… Type-safe, readable, reusable, composable
}

# Assume role policy (who can use this role)
data "aws_iam_policy_document" "assume_role" {
  statement {
    effect  = "Allow"
    actions = ["sts:AssumeRole"]

    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com", "lambda.amazonaws.com"]
    }

    # Conditional assume role (MFA required)
    condition {
      test     = "Bool"
      variable = "aws:MultiFactorAuthPresent"
      values   = ["true"]
    }
  }
}

resource "aws_iam_role" "app" {
  name               = "${var.environment}-app-role"
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}
```

## Edge Cases and Gotchas

```hcl
# ============================================
# GOTCHA 1: Data source caching in plan vs apply
# ============================================
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]
  }
}
# âš ï¸ If a new AMI is published between terraform plan and terraform apply,
# the apply will use the NEW AMI (not the one shown in plan)
# This can cause unexpected instance replacements!

# âœ… Solution: Use -refresh=false flag if you want plan=apply consistency
# terraform apply -refresh=false


# ============================================
# GOTCHA 2: terraform_remote_state tight coupling
# ============================================
# If Team A's output names change, Team B's code breaks!
# This creates deployment dependencies between teams.

# âœ… Better pattern: Use SSM Parameter Store as a loose-coupling layer
# Team A writes:
resource "aws_ssm_parameter" "vpc_id" {
  name  = "/shared/networking/vpc-id"
  type  = "String"
  value = aws_vpc.main.id
}

# Team B reads (no dependency on Team A's state file!):
data "aws_ssm_parameter" "vpc_id" {
  name = "/shared/networking/vpc-id"
}

resource "aws_instance" "app" {
  subnet_id = data.aws_ssm_parameter.vpc_id.value
}


# ============================================
# GOTCHA 3: Data source not found during first apply
# ============================================
# If the resource doesn't exist yet, data source will fail!
# Common in bootstrapping scenarios.

# âŒ This fails on first run if the VPC doesn't exist:
data "aws_vpc" "main" {
  filter {
    name   = "tag:Name"
    values = ["production-vpc"]
  }
}

# âœ… Solution 1: Create the VPC in a separate apply first
# âœ… Solution 2: Use try() function for optional data sources (Terraform 1.1+)
# âœ… Solution 3: Structure your code so VPC is in same root module


# ============================================
# GOTCHA 4: Sensitive values in data sources
# ============================================
data "aws_secretsmanager_secret_version" "db_pass" {
  secret_id = "production/db-password"
}

output "db_password" {
  value     = data.aws_secretsmanager_secret_version.db_pass.secret_string
  sensitive = true   # âœ… Mark as sensitive â€” won't show in terraform output
}

# âš ï¸ Even with sensitive=true, the value IS stored in state file in plaintext!
# Always encrypt your state file backend (S3 with SSE, Terraform Cloud, etc.)


# ============================================
# GOTCHA 5: Using count/for_each with data sources
# ============================================
# When data source uses for_each, all keys must be known at plan time
variable "subnet_names" {
  default = ["web", "api", "db"]
}

data "aws_subnet" "app_subnets" {
  for_each = toset(var.subnet_names)  # âœ… Static set â€” known at plan time

  filter {
    name   = "tag:Name"
    values = ["prod-${each.value}-subnet"]
  }
}

# âŒ This fails â€” for_each key depends on another resource's output:
data "aws_subnet" "bad" {
  for_each = toset(aws_vpc.main.*.id)  # âŒ Not known at plan time
}
```

---

## Performance Considerations

```hcl
# ============================================
# PERFORMANCE: Minimize redundant API calls
# ============================================

# âŒ BAD: Multiple data sources calling same API
data "aws_caller_identity" "for_bucket" {}
data "aws_caller_identity" "for_lambda" {}
data "aws_caller_identity" "for_iam" {}
# Makes 3 identical API calls!

# âœ… GOOD: One data source, reference everywhere
data "aws_caller_identity" "current" {}
locals {
  account_id = data.aws_caller_identity.current.account_id
}
# Use local.account_id everywhere â€” 1 API call


# ============================================
# PERFORMANCE: Use data sources wisely with large for_each
# ============================================
# Looking up 500 subnets individually = 500 API calls = very slow plan
data "aws_subnet" "each_one" {
  for_each = toset(var.500_subnet_names)   # âŒ 500 API calls
}

# âœ… Use the plural form (returns all at once):
data "aws_subnets" "all" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.main.id]
  }
}
# âœ… 1 API call returns all subnet IDs as a list


# ============================================
# PERFORMANCE: -refresh=false flag
# ============================================
# During rapid iteration, skip refreshing data sources:
# terraform plan -refresh=false
# terraform apply -refresh=false
# Uses cached state â€” faster but may miss real-world drift
```

---

## Production Best Practices

```hcl
# ============================================
# PRODUCTION PATTERN 1: Centralized Data Lookups Module
# ============================================

# modules/context/main.tf â€” A "context" module all other modules use
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}
data "aws_partition" "current" {}   # "aws", "aws-cn", "aws-us-gov"

data "aws_vpc" "main" {
  filter {
    name   = "tag:Environment"
    values = [var.environment]
  }
}

data "aws_subnets" "private" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.main.id]
  }
  filter {
    name   = "tag:Tier"
    values = ["private"]
  }
}

# modules/context/outputs.tf
output "account_id"         { value = data.aws_caller_identity.current.account_id }
output "region"             { value = data.aws_region.current.name }
output "partition"          { value = data.aws_partition.current.partition }
output "vpc_id"             { value = data.aws_vpc.main.id }
output "vpc_cidr"           { value = data.aws_vpc.main.cidr_block }
output "private_subnet_ids" { value = data.aws_subnets.private.ids }

# In your root module â€” call context once, use everywhere
module "context" {
  source      = "./modules/context"
  environment = var.environment
}

module "compute" {
  source             = "./modules/compute"
  vpc_id             = module.context.vpc_id
  private_subnet_ids = module.context.private_subnet_ids
  account_id         = module.context.account_id
}

module "database" {
  source             = "./modules/database"
  vpc_id             = module.context.vpc_id
  private_subnet_ids = module.context.private_subnet_ids
}
```

---

# ðŸ—ï¸ REAL-WORLD ARCHITECTURE EXAMPLE

## Complete: Multi-Tier Application Using Data Sources

```hcl
# ============================================
# PRODUCTION: App deployment using existing network
# (Network created by separate Terraform project)
# ============================================

# data.tf â€” All data source lookups in one file

# Who are we?
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}
data "aws_partition" "current" {}

# Find the latest approved Golden AMI (built by our image pipeline)
data "aws_ami" "golden" {
  most_recent = true
  owners      = [data.aws_caller_identity.current.account_id]

  filter {
    name   = "tag:Project"
    values = [var.project_name]
  }

  filter {
    name   = "tag:Status"
    values = ["approved"]
  }

  filter {
    name   = "tag:Environment"
    values = [var.environment, "base"]
  }

  # Validation: Ensure AMI is not older than 30 days
  lifecycle {
    postcondition {
      condition = timecmp(
        self.creation_date,
        timeadd(timestamp(), "-720h")  # 720 hours = 30 days
      ) > 0
      error_message = "Golden AMI is older than 30 days. Run image pipeline first."
    }
  }
}

# Read the networking stack outputs (managed by platform team)
data "terraform_remote_state" "networking" {
  backend = "s3"
  config = {
    bucket = "${var.company_name}-terraform-state"
    key    = "${var.environment}/networking/terraform.tfstate"
    region = "us-east-1"
  }
}

# Read secrets from Secrets Manager
data "aws_secretsmanager_secret_version" "app_secrets" {
  secret_id = "${var.environment}/${var.project_name}/app-config"
}

# SSM Parameters for non-sensitive shared config
data "aws_ssm_parameter" "rds_endpoint" {
  name = "/${var.environment}/shared/rds-endpoint"
}

data "aws_ssm_parameter" "redis_endpoint" {
  name = "/${var.environment}/shared/redis-endpoint"
}

# Get existing security groups managed by security team
data "aws_security_group" "internal_access" {
  filter {
    name   = "tag:Name"
    values = ["${var.environment}-internal-access"]
  }
}

# ============================================
# locals.tf â€” Process data source results
# ============================================
locals {
  account_id  = data.aws_caller_identity.current.account_id
  region      = data.aws_region.current.name
  partition   = data.aws_partition.current.partition

  # Parse secrets
  app_secrets = jsondecode(
    data.aws_secretsmanager_secret_version.app_secrets.secret_string
  )

  # Combine networking outputs
  vpc_id             = data.terraform_remote_state.networking.outputs.vpc_id
  private_subnet_ids = data.terraform_remote_state.networking.outputs.private_subnet_ids
  public_subnet_ids  = data.terraform_remote_state.networking.outputs.public_subnet_ids
  alb_arn            = data.terraform_remote_state.networking.outputs.alb_arn

  # Build app configuration for user_data script
  app_config = {
    environment   = var.environment
    project       = var.project_name
    db_host       = data.aws_ssm_parameter.rds_endpoint.value
    redis_host    = data.aws_ssm_parameter.redis_endpoint.value
    api_key       = local.app_secrets.api_key
    feature_flags = local.app_secrets.feature_flags
  }

  common_tags = {
    Project     = var.project_name
    Environment = var.environment
    ManagedBy   = "terraform"
    AMI         = data.aws_ami.golden.id
    DeployedAt  = timestamp()
  }
}

# ============================================
# compute.tf â€” Use all the data source lookups
# ============================================

# IAM Policy using policy_document data source
data "aws_iam_policy_document" "app_role_policy" {
  statement {
    sid     = "SecretsAccess"
    effect  = "Allow"
    actions = ["secretsmanager:GetSecretValue"]
    resources = [
      "arn:${local.partition}:secretsmanager:${local.region}:${local.account_id}:secret:${var.environment}/${var.project_name}/*"
    ]
  }

  statement {
    sid     = "SSMAccess"
    effect  = "Allow"
    actions = ["ssm:GetParameter", "ssm:GetParameters", "ssm:GetParametersByPath"]
    resources = [
      "arn:${local.partition}:ssm:${local.region}:${local.account_id}:parameter/${var.environment}/*"
    ]
  }

  statement {
    sid     = "S3AppData"
    effect  = "Allow"
    actions = ["s3:GetObject", "s3:PutObject"]
    resources = [
      "arn:${local.partition}:s3:::${var.environment}-${var.project_name}-data/*"
    ]
  }
}

data "aws_iam_policy_document" "ec2_assume_role" {
  statement {
    effect  = "Allow"
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

resource "aws_iam_role" "app" {
  name               = "${var.environment}-${var.project_name}-role"
  assume_role_policy = data.aws_iam_policy_document.ec2_assume_role.json
  tags               = local.common_tags
}

resource "aws_iam_role_policy" "app" {
  name   = "${var.environment}-${var.project_name}-policy"
  role   = aws_iam_role.app.id
  policy = data.aws_iam_policy_document.app_role_policy.json
}

resource "aws_launch_template" "app" {
  name_prefix   = "${var.environment}-${var.project_name}-"
  image_id      = data.aws_ami.golden.id       # â† From data source
  instance_type = var.instance_type

  iam_instance_profile {
    arn = aws_iam_instance_profile.app.arn
  }

  network_interfaces {
    associate_public_ip_address = false
    subnet_id                   = local.private_subnet_ids[0]  # â† From remote state
    security_groups = [
      aws_security_group.app.id,
      data.aws_security_group.internal_access.id               # â† From data source
    ]
  }

  user_data = base64encode(templatefile("${path.module}/templates/userdata.sh.tpl",
    local.app_config
  ))

  metadata_options {
    http_endpoint               = "enabled"
    http_tokens                 = "required"  # IMDSv2 only
    http_put_response_hop_limit = 1
  }

  lifecycle {
    create_before_destroy = true
  }

  tags = local.common_tags
}

resource "aws_autoscaling_group" "app" {
  name                = "${var.environment}-${var.project_name}-asg"
  vpc_zone_identifier = local.private_subnet_ids    # â† From remote state

  min_size         = var.min_instances
  max_size         = var.max_instances
  desired_capacity = var.min_instances

  launch_template {
    id      = aws_launch_template.app.id
    version = "$Latest"
  }

  lifecycle {
    ignore_changes = [desired_capacity]
  }
}
```

---

## Debugging Data Sources

```bash
# ============================================
# DEBUGGING TOOLKIT FOR DATA SOURCES
# ============================================

# 1. See what a data source returned during plan
terraform plan -out=tfplan
terraform show tfplan | grep -A 20 "data\."

# 2. Interactive exploration â€” test filters before using in code
terraform console

# In console:
> data.aws_ami.ubuntu.id
"ami-0a91cd140a1fc943b"

> data.aws_ami.ubuntu.name
"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-20240301"

> data.aws_vpc.main
{
  "arn"        = "arn:aws:ec2:us-east-1:123456789:vpc/vpc-abc123"
  "cidr_block" = "10.0.0.0/16"
  "id"         = "vpc-abc123"
  ...
}

> data.aws_subnets.private.ids
tolist(["subnet-abc", "subnet-def", "subnet-ghi"])

> jsondecode(data.aws_secretsmanager_secret_version.app.secret_string)
{ "api_key" = "...", "db_pass" = "..." }

# 3. Debug "no matching resource found" errors
# Add TF_LOG to see the actual API call and response:
TF_LOG=DEBUG terraform plan 2>&1 | grep -A 5 "DescribeImages\|DescribeVpcs"

# 4. Check what attributes are available from a data source
terraform console
> data.aws_ami.ubuntu   # Prints ALL attributes and their values

# 5. Test filter values manually using AWS CLI
aws ec2 describe-images \
  --owners amazon \
  --filters "Name=name,Values=amzn2-ami-hvm-*-x86_64-gp2" \
            "Name=state,Values=available" \
  --query 'sort_by(Images, &CreationDate)[-1].[ImageId,Name]'
# This shows exactly what Terraform's data source will return

# 6. Refresh data sources in isolation
terraform refresh   # Re-reads all data sources and updates state
terraform plan -refresh-only  # Show what would change if you refresh
```

---

# ðŸ§ª HANDS-ON LAB

## Lab: Multi-Source Configuration Builder

**Objective:** Practice data sources by building a configuration file system that reads from multiple sources and merges them into unified outputs.

**Requirements â€” no cloud needed, uses local + http providers:**

1. Use `data "local_file"` to read an existing base config file
2. Use `data "http"` to fetch a public API (JSONPlaceholder)
3. Use `data "aws_caller_identity"` equivalent â€” use `data "http"` to get your public IP
4. Combine all data into a unified JSON config file using `local_file` resource
5. Use `templatefile()` to render an HTML report from the combined data
6. Output a summary of what was read from each source

**Expected output:**
```
Apply complete! Resources: 2 added, 0 changed, 0 destroyed.

Outputs:

data_summary = {
  "base_config_lines" = 4
  "api_user_name"     = "Leanne Graham"
  "public_ip"         = "203.0.113.42"
  "generated_files"   = ["./output/unified-config.json", "./output/report.html"]
}
```

**Try it yourself before looking at the solution!**

---

## Lab Solution

```hcl
# ============================================
# FILE STRUCTURE:
# .
# â”œâ”€â”€ main.tf
# â”œâ”€â”€ variables.tf
# â”œâ”€â”€ outputs.tf
# â”œâ”€â”€ templates/
# â”‚   â””â”€â”€ report.html.tpl
# â”œâ”€â”€ base-config.txt       â† you create this
# â””â”€â”€ output/               â† Terraform creates this
# ============================================

# First, create base-config.txt manually:
# environment=development
# version=1.0.0
# debug=true
# log_level=INFO


# main.tf
terraform {
  required_providers {
    local = {
      source  = "hashicorp/local"
      version = "~> 2.4"
    }
    http = {
      source  = "hashicorp/http"
      version = "~> 3.0"
    }
  }
}

# DATA SOURCE 1: Read base config file
data "local_file" "base_config" {
  filename = "${path.module}/base-config.txt"
}

# DATA SOURCE 2: Fetch user data from public API
data "http" "api_user" {
  url = "https://jsonplaceholder.typicode.com/users/1"

  request_headers = {
    Accept = "application/json"
  }
}

# DATA SOURCE 3: Get public IP address
data "http" "public_ip" {
  url = "https://ipv4.icanhazip.com"
}

# Process data in locals
locals {
  # Parse the API response
  api_user = jsondecode(data.http.api_user.response_body)

  # Clean up IP (remove trailing newline)
  public_ip = chomp(data.http.public_ip.response_body)

  # Parse base config lines
  config_lines = [
    for line in split("\n", trimspace(data.local_file.base_config.content)) :
    line if line != ""
  ]

  # Build unified config
  unified_config = {
    generated_at = timestamp()
    source_ip    = local.public_ip
    base_config  = { for line in local.config_lines :
      split("=", line)[0] => split("=", line)[1]
    }
    api_data = {
      user_id   = local.api_user.id
      user_name = local.api_user.name
      email     = local.api_user.email
      company   = local.api_user.company.name
    }
  }
}

# RESOURCE 1: Write unified JSON config
resource "local_file" "unified_config" {
  filename = "./output/unified-config.json"
  content  = jsonencode(local.unified_config)
}

# RESOURCE 2: Render HTML report from template
resource "local_file" "report" {
  filename = "./output/report.html"
  content = templatefile("${path.module}/templates/report.html.tpl", {
    config     = local.unified_config
    api_user   = local.api_user
    public_ip  = local.public_ip
    config_raw = data.local_file.base_config.content
  })
}

# outputs.tf
output "data_summary" {
  value = {
    base_config_lines = length(local.config_lines)
    api_user_name     = local.api_user.name
    public_ip         = local.public_ip
    generated_files   = [
      local_file.unified_config.filename,
      local_file.report.filename,
    ]
  }
}


# templates/report.html.tpl
# <!DOCTYPE html>
# <html>
# <head><title>Terraform Data Sources Report</title></head>
# <body>
#   <h1>Infrastructure Report</h1>
#   <p>Generated from ${length(split("\n", config_raw))} config lines</p>
#   <h2>API User</h2>
#   <p>Name: ${api_user.name}</p>
#   <p>Email: ${api_user.email}</p>
#   <h2>Network Info</h2>
#   <p>Public IP: ${public_ip}</p>
# </body>
# </html>
```

```bash
# Setup
mkdir -p output templates

cat > base-config.txt <<EOF
environment=development
version=1.0.0
debug=true
log_level=INFO
EOF

cat > templates/report.html.tpl <<'EOF'
<!DOCTYPE html>
<html>
<head><title>Data Sources Report</title></head>
<body>
  <h1>Infrastructure Report</h1>
  <h2>API User: ${api_user.name}</h2>
  <p>Email: ${api_user.email}</p>
  <p>Company: ${api_user.company.name}</p>
  <h2>Network</h2>
  <p>Runner IP: ${public_ip}</p>
  <h2>Base Config</h2>
  <pre>${config_raw}</pre>
</body>
</html>
EOF

# Run
terraform init
terraform plan
terraform apply -auto-approve

# Explore outputs
cat output/unified-config.json | python3 -m json.tool
open output/report.html  # or: cat output/report.html
```

---

# ðŸ“‹ SUMMARY CHEAT SHEET

## Key Concepts

| Concept | Details |
|---|---|
| Purpose | Read-only lookup of existing infrastructure |
| Syntax | `data "<type>" "<name>" { }` |
| Reference | `data.<type>.<name>.<attribute>` |
| Evaluated | At plan time (or apply time if depends on uncreated resource) |
| Fails when | 0 results or 2+ results match filters |
| Never does | Create, update, or destroy infrastructure |

---

## Quick Syntax Reference

```hcl
# Basic data source
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]
  filter {
    name   = "name"
    values = ["ubuntu-*"]
  }
}

# Reference it
resource "aws_instance" "web" {
  ami = data.aws_ami.ubuntu.id
}

# With for_each
data "aws_subnet" "app" {
  for_each = toset(["web", "api"])
  filter {
    name   = "tag:Name"
    values = ["${each.value}-subnet"]
  }
}

# Remote state
data "terraform_remote_state" "network" {
  backend = "s3"
  config = {
    bucket = "tf-state-bucket"
    key    = "network/terraform.tfstate"
    region = "us-east-1"
  }
}
output "vpc" {
  value = data.terraform_remote_state.network.outputs.vpc_id
}

# IAM policy document
data "aws_iam_policy_document" "policy" {
  statement {
    effect    = "Allow"
    actions   = ["s3:GetObject"]
    resources = ["arn:aws:s3:::my-bucket/*"]
  }
}

# Common account/region lookups
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}
data "aws_availability_zones" "available" { state = "available" }

# Secrets
data "aws_secretsmanager_secret_version" "secret" {
  secret_id = "prod/myapp/creds"
}
locals {
  creds = jsondecode(data.aws_secretsmanager_secret_version.secret.secret_string)
}
```

---

## Common Data Sources Quick Reference

```
aws_ami                          â†’ Find EC2 machine image
aws_vpc                          â†’ Find a VPC
aws_subnet / aws_subnets         â†’ Find subnet(s)
aws_security_group               â†’ Find a security group
aws_caller_identity              â†’ Current account ID / ARN
aws_region                       â†’ Current region
aws_availability_zones           â†’ Available AZs
aws_partition                    â†’ aws / aws-cn / aws-us-gov
aws_secretsmanager_secret_version â†’ Fetch a secret
aws_ssm_parameter                â†’ Fetch SSM parameter
aws_iam_policy_document          â†’ Generate IAM policy JSON
aws_route53_zone                 â†’ Find hosted zone
aws_acm_certificate              â†’ Find SSL certificate
aws_lb / aws_lb_listener         â†’ Find load balancer
terraform_remote_state           â†’ Read another project's outputs
local_file                       â†’ Read a local file
http                             â†’ Fetch from HTTP endpoint
```

---

## Top Interview Questions on Data Sources

**Q1: What is a data source and when would you use one instead of a resource?**
> A data source reads existing infrastructure without managing it. Use a data source when the infrastructure was created outside your Terraform project, by another team, or by a separate state file. Use a resource when Terraform should own the full lifecycle.

**Q2: What happens if your data source filter matches multiple resources?**
> Terraform throws an error: "multiple resources found." You must add more specific filters to narrow it to exactly one result, or use a plural data source like `aws_subnets` that is designed to return multiple results.

**Q3: How do you share data between two separate Terraform projects?**
> Three patterns: `terraform_remote_state` reads another project's state outputs directly; AWS SSM Parameter Store / Secrets Manager acts as a loosely-coupled data layer; or a shared data store like Consul. Remote state is simplest but creates deployment coupling. SSM is preferred for team-scale architectures.

**Q4: Why should you use a data source for AMI IDs instead of hardcoding them?**
> AMI IDs are region-specific, change when new versions are released, and become unavailable over time. A data source with `most_recent = true` always fetches the current valid AMI for the correct region automatically.

**Q5: What is aws_iam_policy_document and why is it better than jsonencode?**
> It's a data source that generates IAM policy JSON. It's better than `jsonencode` because it validates structure at plan time, supports merging multiple policy documents with `source_policy_documents`, and produces readable HCL instead of raw JSON strings.

**Q6: How do data sources handle sensitive values?**
> Data sources can return sensitive values (like secrets). Mark outputs using `sensitive = true` to prevent them from displaying in CLI output. However, values are still stored in the state file in plaintext â€” always encrypt your backend.

**Q7: Can data sources use for_each?**
> Yes, data sources support `for_each` just like resources. The for_each keys must be known at plan time (static maps or sets). This lets you look up multiple resources dynamically from a single data block.

---
